# SparkWithScala
Here, In this repo we will discuss spark with scala by sbt

Spark With Scala


Try to Cover all scenario based hands on
Try to use TypeSafe and others things too. 

Spark RDD (Low-Level Construct)

- Spark RDD - Parallelize ( To convert any type of collection to rdd ) ✔
- Spark RDD - Reading Files ✔
- Spark RDD - Create RDD ✔
- Spark RDD - Create empty RDD ✔
- Spark RDD - Transformations ✔
- Spark RDD - Actions ✔
- Spark RDD - Pair Functions ✔
- Spark RDD - Repartition and Coalesce ✔
- Spark RDD - Shuffle Partitions ✔
- Spark RDD - Cache vs Persist ✔
- Spark RDD - Persistance Storage Levels ✔
- Spark RDD - Broadcast Variables ✔
- Spark RDD - Accumulator Variables
- Spark RDD - Convert RDD to Dataframe ✔

Optimization in Low-level Construct (RDD)

- Using Cache and Persist
- Doing broadcast() as per scenario --- Which avoid shuffling
- Adjusting shuffle partitions using coalesce and repartition whenever required.
- File Format Selection
- Serialization

------------------------------------------------------------------------------------------------------------------------
